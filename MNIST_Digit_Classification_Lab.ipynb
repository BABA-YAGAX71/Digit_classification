{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\uddea MNIST Handwritten Digit Classification Lab Final\n", "\n", "**Dataset**: MNIST (70,000 handwritten digit images)\n", "\n", "**Tasks:**\n", "1. Dataset Acquisition\n", "2. Data Loading\n", "3. Data Exploration\n", "4. Data Visualization\n", "5. Data Preprocessing\n", "6. Model Training (Logistic Regression, SVM, Decision Tree)\n", "7. Hyperparameter Optimization\n", "8. Training Evaluation\n", "9. Testing Evaluation with Bootstrapping\n", "10. Model Comparison"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install required libraries (uncomment if needed)\n", "# !pip install scikit-learn matplotlib seaborn numpy\n", "from sklearn.datasets import fetch_openml\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n", "from sklearn.utils import resample\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udce5 1. Dataset Acquisition and Loading"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load MNIST from OpenML\n", "mnist = fetch_openml('mnist_784', version=1)\n", "X, y = mnist.data, mnist.target.astype('int')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd0d 2. Data Exploration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Number of samples:\", X.shape[0])\n", "print(\"Number of features:\", X.shape[1])\n", "print(\"Target classes:\", np.unique(y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\uddbc\ufe0f 3. Data Visualization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Display 10 sample digits\n", "fig, axes = plt.subplots(1, 10, figsize=(12, 4))\n", "for i in range(10):\n", "    axes[i].imshow(X.iloc[i].values.reshape(28, 28), cmap='gray')\n", "    axes[i].axis('off')\n", "    axes[i].set_title(f\"Label: {y[i]}\")\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddf9 4. Data Preprocessing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Normalize and split data\n", "scaler = StandardScaler()\n", "X_scaled = scaler.fit_transform(X)\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udd16 5. Model Training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize models\n", "lr = LogisticRegression(max_iter=1000)\n", "svm = SVC()\n", "dt = DecisionTreeClassifier()\n", "\n", "# Fit models\n", "lr.fit(X_train, y_train)\n", "svm.fit(X_train, y_train)\n", "dt.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd0d 6. Hyperparameter Tuning (GridSearchCV)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Logistic Regression\n", "param_lr = {'C': [0.1, 1, 10]}\n", "grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_lr, cv=3)\n", "grid_lr.fit(X_train, y_train)\n", "\n", "# SVM\n", "param_svm = {'C': [0.1, 1], 'kernel': ['linear', 'rbf']}\n", "grid_svm = GridSearchCV(SVC(), param_svm, cv=3)\n", "grid_svm.fit(X_train, y_train)\n", "\n", "# Decision Tree\n", "param_dt = {'max_depth': [10, 20, None]}\n", "grid_dt = GridSearchCV(DecisionTreeClassifier(), param_dt, cv=3)\n", "grid_dt.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcca 7. Training Evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(model, X, y):\n", "    y_pred = model.predict(X)\n", "    print(\"Accuracy:\", accuracy_score(y, y_pred))\n", "    print(\"Precision:\", precision_score(y, y_pred, average='weighted'))\n", "    print(\"Recall:\", recall_score(y, y_pred, average='weighted'))\n", "    print(\"F1 Score:\", f1_score(y, y_pred, average='weighted'))\n", "    sns.heatmap(confusion_matrix(y, y_pred), annot=False, fmt='d')\n", "    plt.title(\"Confusion Matrix\")\n", "    plt.show()\n", "    print(classification_report(y, y_pred))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Logistic Regression\")\n", "evaluate(grid_lr.best_estimator_, X_train, y_train)\n", "\n", "print(\"SVM\")\n", "evaluate(grid_svm.best_estimator_, X_train, y_train)\n", "\n", "print(\"Decision Tree\")\n", "evaluate(grid_dt.best_estimator_, X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddea 8. Test Evaluation with Bootstrapping"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def bootstrap(model, X, y, n_iterations=100):\n", "    acc, prec, rec = [], [], []\n", "    for _ in range(n_iterations):\n", "        X_s, y_s = resample(X, y)\n", "        y_pred = model.predict(X_s)\n", "        acc.append(accuracy_score(y_s, y_pred))\n", "        prec.append(precision_score(y_s, y_pred, average='weighted'))\n", "        rec.append(recall_score(y_s, y_pred, average='weighted'))\n", "    return np.mean(acc), np.std(acc), np.mean(prec), np.std(prec), np.mean(rec), np.std(rec)\n", "\n", "models = {'LR': grid_lr.best_estimator_, 'SVM': grid_svm.best_estimator_, 'DT': grid_dt.best_estimator_}\n", "for name, model in models.items():\n", "    print(f\"\\n{name} Bootstrapping Results\")\n", "    acc_m, acc_s, p_m, p_s, r_m, r_s = bootstrap(model, X_test, y_test)\n", "    print(f\"Accuracy: {acc_m:.4f} \u00b1 {1.96*acc_s:.4f}\")\n", "    print(f\"Precision: {p_m:.4f} \u00b1 {1.96*p_s:.4f}\")\n", "    print(f\"Recall: {r_m:.4f} \u00b1 {1.96*r_s:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2705 9. Model Comparison and Conclusion"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Write your conclusion here:\n", "# - Which model performed best?\n", "# - Did any model overfit?\n", "# - What would you improve next time?"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}